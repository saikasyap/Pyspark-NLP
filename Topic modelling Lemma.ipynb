{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling using LDA with Lemmitizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "import sparknlp\n",
    "from sparknlp import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "%matplotlib inline\n",
    "from sparknlp.annotator import PerceptronModel\n",
    "spark = sparknlp.start()\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.feature import CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
    "from sparknlp.annotator import NGramGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"C:/Users/saika/Desktop/Pyspark/data/shopee_reviews.csv\"\n",
    "df = spark.read.csv(dataPath, header='true', inferSchema = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               label|                text|\n",
      "+--------------------+--------------------+\n",
      "|                   5|Looks ok. Not lik...|\n",
      "|                   5|Tried, the curren...|\n",
      "|                   5|Item received aft...|\n",
      "|                   5|Thanks!!! Works a...|\n",
      "|                   5|Fast delivery con...|\n",
      "|                   5|Fast delivery goo...|\n",
      "|                   5|Got my order and ...|\n",
      "|                   5|Items received in...|\n",
      "|                   5|Received in good ...|\n",
      "|                   1|Item doesn’t work . |\n",
      "|Asked me to send ...| show a non worki...|\n",
      "|Don’t waste time ...|                null|\n",
      "|                   5|         Fast. Great|\n",
      "|                   5|I've tried it, an...|\n",
      "|                   5|Hub uses it. Musc...|\n",
      "|                   5|Well received. Fa...|\n",
      "|                   5|Product received....|\n",
      "|                   5|             Good.. |\n",
      "|                   5|box was a little ...|\n",
      "|                   4|Fast delivery, ho...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|                                                                                      text|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|   Looks ok. Not like so durable. Will hv to use a while to recommend others of its worth.|\n",
      "|Tried, the current can be very powerful depending on the setting, i don't dare to go hi...|\n",
      "|               Item received after a week. Looks smaller than expected, can’t wait to try!|\n",
      "|Thanks!!! Works as describe no complaints. Not really expecting any life changing resul...|\n",
      "|Fast delivery considering it’s from overseas and only tried once. Not sure about the re...|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna()\n",
    "text_col = 'text'\n",
    "\n",
    "text = df.select(text_col).filter(F.col(text_col).isNotNull())\n",
    "\n",
    "text.limit(5).show(truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer = LemmatizerModel.pretrained()\\\n",
    "#   .setInputCols(['token'])\\\n",
    "#    .setOutputCol('lemma')\n",
    "\n",
    "#norvig_pretrained = NorvigSweetingModel.pretrained()\\\n",
    " #   .setInputCols(['tokens_filtered'])\\\n",
    "  #  .setOutputCol('norvig_annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "assembler = DocumentAssembler()\\\n",
    "    .setInputCol('text')\\\n",
    "    .setOutputCol('document')\n",
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentences\")\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(['sentences'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "#stemmer = Stemmer()\\\n",
    " #   .setInputCols(['token'])\\\n",
    "  #  .setOutputCol('stem')\n",
    "lemmatizer = LemmatizerModel.pretrained()\\\n",
    "   .setInputCols(['token'])\\\n",
    "    .setOutputCol('lemma')\n",
    "normalizer = Normalizer()\\\n",
    "    .setCleanupPatterns([\n",
    "        '[^a-zA-Z.-]+', \n",
    "        '^[^a-zA-Z]+', \n",
    "        '[^a-zA-Z]+$',\n",
    "    ])\\\n",
    "    .setInputCols(['token'])\\\n",
    "    .setOutputCol('normalized')\\\n",
    "    .setLowercase(True)\n",
    "sw_clean = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemma']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 4.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
    "    .setInputCols(['document', 'lemma']) \\\n",
    "    .setOutputCol('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "     .setInputCols(['unigrams', 'ngrams', 'pos']) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "     .setStages([assembler, sentence, tokenizer, lemmatizer, normalizer, sw_clean, ngrammer,pos_tagger, finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pipeline.fit(text).transform(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|Looks ok. Not lik...|[Looks, ok, ., li...|[Looks, ok, ., Lo...|[NNP, JJ, ., RB, ...|\n",
      "|Tried, the curren...|[Tried, ,, curren...|[Tried, ,, the, c...|[NNP, ,, DT, JJ, ...|\n",
      "|Item received aft...|[Item, receive, w...|[Item, receive, a...|[NNP, VB, IN, DT,...|\n",
      "|Thanks!!! Works a...|[Thanks, !!!, Wor...|[Thanks, !!!, Tha...|[NNS, ., NNP, IN,...|\n",
      "|Fast delivery con...|[Fast, delivery, ...|[Fast, delivery, ...|[RB, NN, NN, NN, ...|\n",
      "|Fast delivery goo...|[Fast, delivery, ...|[Fast, delivery, ...|    [RB, NN, JJ, NN]|\n",
      "|Got my order and ...|[Got, order, come...|[Got, i, order, a...|[NNP, NNP, NN, CC...|\n",
      "|Items received in...|[Items, receive, ...|[Items, receive, ...|[NNS, VBP, IN, DT...|\n",
      "|Received in good ...|[Received, good, ...|[Received, in, go...|[NNP, IN, JJ, NN,...|\n",
      "|Item doesn’t work . |[Item, doesn’t, w...|[Item, doesn’t, w...|    [NNP, JJ, NN, .]|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed.limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "tf = CountVectorizer(inputCol='finished_unigrams', outputCol='tf_features')\n",
    "tf_model = tf.fit(processed)\n",
    "tf_result = tf_model.transform(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         tf_features|\n",
      "+--------------------+\n",
      "|(189545,[0,25,34,...|\n",
      "|(189545,[0,2,7,13...|\n",
      "|(189545,[0,2,3,5,...|\n",
      "|(189545,[0,3,33,3...|\n",
      "|(189545,[0,4,15,2...|\n",
      "|(189545,[1,4,15,1...|\n",
      "|(189545,[0,1,3,8,...|\n",
      "|(189545,[0,2,3,5,...|\n",
      "|(189545,[0,1,2,6,...|\n",
      "|(189545,[0,14,27,...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_result.select('tf_features').limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|   finished_unigrams|     finished_ngrams|        finished_pos|         tf_features|     tf_idf_features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Looks ok. Not lik...|[Looks, ok, ., li...|[Looks, ok, ., Lo...|[NNP, JJ, ., RB, ...|(189545,[0,25,34,...|(189545,[0,25,34,...|\n",
      "|Tried, the curren...|[Tried, ,, curren...|[Tried, ,, the, c...|[NNP, ,, DT, JJ, ...|(189545,[0,2,7,13...|(189545,[0,2,7,13...|\n",
      "|Item received aft...|[Item, receive, w...|[Item, receive, a...|[NNP, VB, IN, DT,...|(189545,[0,2,3,5,...|(189545,[0,2,3,5,...|\n",
      "|Thanks!!! Works a...|[Thanks, !!!, Wor...|[Thanks, !!!, Tha...|[NNS, ., NNP, IN,...|(189545,[0,3,33,3...|(189545,[0,3,33,3...|\n",
      "|Fast delivery con...|[Fast, delivery, ...|[Fast, delivery, ...|[RB, NN, NN, NN, ...|(189545,[0,4,15,2...|(189545,[0,4,15,2...|\n",
      "|Fast delivery goo...|[Fast, delivery, ...|[Fast, delivery, ...|    [RB, NN, JJ, NN]|(189545,[1,4,15,1...|(189545,[1,4,15,1...|\n",
      "|Got my order and ...|[Got, order, come...|[Got, i, order, a...|[NNP, NNP, NN, CC...|(189545,[0,1,3,8,...|(189545,[0,1,3,8,...|\n",
      "|Items received in...|[Items, receive, ...|[Items, receive, ...|[NNS, VBP, IN, DT...|(189545,[0,2,3,5,...|(189545,[0,2,3,5,...|\n",
      "|Received in good ...|[Received, good, ...|[Received, in, go...|[NNP, IN, JJ, NN,...|(189545,[0,1,2,6,...|(189545,[0,1,2,6,...|\n",
      "|Item doesn’t work . |[Item, doesn’t, w...|[Item, doesn’t, w...|    [NNP, JJ, NN, .]|(189545,[0,14,27,...|(189545,[0,14,27,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_result.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 5\n",
    "max_iter = 10\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features')\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5 topics with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------------+\n",
      "|topic|                                                          topicWords|\n",
      "+-----+--------------------------------------------------------------------+\n",
      "|    0|               [., !, ,, good, receive, item, fast, week, buy, take]|\n",
      "|    1|    [., !, good, ,, delivery, quality, Fast, fast, condition, price]|\n",
      "|    2|[., good, !, ,, material, receive, Thanks, condition, Item, picture]|\n",
      "|    3|       [., ,, receive, !, order, seller, delivery, well, good, time]|\n",
      "|    4|          [., ,, good, !, product, buy, small, size, use, condition]|\n",
      "+-----+--------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 10\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
